# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vpzo0vh4Uq4-03CUC4KVktbA3_1WkrGC
"""

!pip install openai

!pip install transformers

!pip install streamlit
!streamlit hello
!pip install tokenizers

#imports
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import re
from google.colab import drive
import random
#gpt2
from transformers import GPT2Tokenizer
from transformers import GPT2LMHeadModel, GPT2Config
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup

#pytorch
import torch
from torch.utils.data import Dataset
from torch.utils.data import random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
import stramlit as st

drive.mount('/content/drive')

# load the model from disk

import pickle
filename = '/content/drive/MyDrive/Colab Notebooks/finalized_model.sav'

loaded_model = pickle.load(open(filename, 'rb'))
print(loaded_model)

tweet_2012 = '<|sos|>'
loaded_model.eval()

prompt = tweet_2012
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', 
                                          bos_token='<|sos|>', 
                                          eos_token='<|eos|>', 
                                          pad_token='<|pad|>')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
         
generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

outputs = loaded_model.generate(
                                generated, 
                                do_sample=True,   
                                top_k=50, 
                                max_length = 250,
                                top_p=0.95, 
                                num_return_sequences=5
                                )

for i, outputs in enumerate(outputs):
    print("{}: {}\n\n".format(i, tokenizer.decode(outputs, skip_special_tokens=True)))

